# test.py
import os
import sys
from pathlib import Path
import torch
from transformers import AutoTokenizer, AutoConfig
import logging
import traceback

# Setup logging v·ªõi format chi ti·∫øt
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(filename)s:%(lineno)d - %(message)s'
)
logger = logging.getLogger(__name__)

# L·∫•y th√¥ng tin file hi·ªán t·∫°i
current_file = Path(__file__).name
current_dir = Path(__file__).parent.absolute()
logger.info(f"üîç ƒêang ch·∫°y file: {current_file}")
logger.info(f"üìÅ Th∆∞ m·ª•c hi·ªán t·∫°i: {current_dir}")

# Add project root to path
project_root = current_dir.parent.parent
sys.path.insert(0, str(project_root))
logger.info(f"üîó Th√™m v√†o sys.path: {project_root}")

# Ki·ªÉm tra c√°c file c·∫ßn thi·∫øt
required_files = [
    current_dir / "vphoberttagger" / "models.py",
    current_dir / "vphoberttagger" / "constant.py", 
    current_dir / "vphoberttagger" / "helper.py"
]

for file_path in required_files:
    if file_path.exists():
        logger.info(f"‚úÖ T√¨m th·∫•y: {file_path}")
    else:
        logger.error(f"‚ùå KH√îNG t√¨m th·∫•y: {file_path}")

try:
    logger.info("üîÑ ƒêang import c√°c module...")
    from model.VPhoBertTaggermaster.vphoberttagger.models import PhoBertSoftmax
    logger.info("‚úÖ Import PhoBertSoftmax th√†nh c√¥ng")
    
    from model.VPhoBertTaggermaster.vphoberttagger.constant import LABEL_MAPPING
    logger.info("‚úÖ Import LABEL_MAPPING th√†nh c√¥ng")
    
    from model.VPhoBertTaggermaster.vphoberttagger.helper import normalize_text
    logger.info("‚úÖ Import normalize_text th√†nh c√¥ng")
    
except ImportError as e:
    logger.error(f"‚ùå IMPORT ERROR trong file {current_file}:")
    logger.error(f"   Chi ti·∫øt l·ªói: {str(e)}")
    logger.error(f"   Traceback:")
    traceback.print_exc()
    logger.error(f"üîß C·∫ßn ki·ªÉm tra:")
    logger.error(f"   1. ƒê∆∞·ªùng d·∫´n sys.path: {sys.path[:3]}...")
    logger.error(f"   2. C·∫•u tr√∫c th∆∞ m·ª•c vphoberttagger/")
    logger.error(f"   3. C√°c file .py trong vphoberttagger/")
    sys.exit(1)
except Exception as e:
    logger.error(f"‚ùå L·ªñI KH√ÅC khi import trong file {current_file}:")
    logger.error(f"   Chi ti·∫øt: {str(e)}")
    traceback.print_exc()
    sys.exit(1)

# Global variables
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = None
tokenizer = None
id2label = None

def load_model():
    """Load model v√† tokenizer m·ªôt l·∫ßn duy nh·∫•t"""
    global model, tokenizer, id2label
    
    if model is not None:
        logger.info("‚úÖ Model ƒë√£ ƒë∆∞·ª£c load tr∆∞·ªõc ƒë√≥")
        return True
    
    try:
        # ƒê∆∞·ªùng d·∫´n t·ªõi model
        model_path = current_dir / "outputs" / "best_model.pt"
        phobert_path = current_dir / "models" / "phobert-base-v2"
        
        logger.info(f"üîç T√¨m ki·∫øm model t·∫°i: {model_path}")
        logger.info(f"üîç T√¨m ki·∫øm PhoBERT t·∫°i: {phobert_path}")
        
        if not model_path.exists():
            logger.error(f"‚ùå FILE NOT FOUND - Model file trong {current_file}:")
            logger.error(f"   ƒê∆∞·ªùng d·∫´n: {model_path}")
            logger.error(f"   Th∆∞ m·ª•c outputs t·ªìn t·∫°i: {model_path.parent.exists()}")
            if model_path.parent.exists():
                logger.error(f"   C√°c file trong outputs/: {list(model_path.parent.glob('*'))}")
            return False
            
        if not phobert_path.exists():
            logger.error(f"‚ùå FOLDER NOT FOUND - PhoBERT folder trong {current_file}:")
            logger.error(f"   ƒê∆∞·ªùng d·∫´n: {phobert_path}")
            logger.error(f"   Th∆∞ m·ª•c models t·ªìn t·∫°i: {phobert_path.parent.exists()}")
            if phobert_path.parent.exists():
                logger.error(f"   C√°c th∆∞ m·ª•c trong models/: {list(phobert_path.parent.glob('*'))}")
            return False
        
        logger.info(f"üîÑ ƒêang load checkpoint t·ª´: {model_path}")
        checkpoint = torch.load(model_path, map_location=device, weights_only=False)
        
        # Ki·ªÉm tra checkpoint
        if 'args' not in checkpoint:
            logger.error(f"‚ùå CHECKPOINT ERROR trong {current_file}:")
            logger.error(f"   Checkpoint kh√¥ng c√≥ 'args'")
            logger.error(f"   C√°c keys trong checkpoint: {list(checkpoint.keys())}")
            return False
        
        if 'model' not in checkpoint:
            logger.error(f"‚ùå CHECKPOINT ERROR trong {current_file}:")
            logger.error(f"   Checkpoint kh√¥ng c√≥ 'model'")
            logger.error(f"   C√°c keys trong checkpoint: {list(checkpoint.keys())}")
            return False
            
        args = checkpoint['args']
        args.model_name_or_path = str(phobert_path)
        
        # Ki·ªÉm tra args
        if not hasattr(args, 'label2id'):
            logger.error(f"‚ùå ARGS ERROR trong {current_file}:")
            logger.error(f"   args kh√¥ng c√≥ 'label2id'")
            logger.error(f"   C√°c thu·ªôc t√≠nh trong args: {dir(args)}")
            return False
        
        if not hasattr(args, 'id2label'):
            logger.error(f"‚ùå ARGS ERROR trong {current_file}:")
            logger.error(f"   args kh√¥ng c√≥ 'id2label'")
            return False
        
        logger.info(f"üîÑ ƒêang load AutoConfig...")
        config = AutoConfig.from_pretrained(args.model_name_or_path, num_labels=len(args.label2id))
        
        logger.info(f"üîÑ ƒêang kh·ªüi t·∫°o model...")
        model = PhoBertSoftmax(config=config)
        
        logger.info(f"üîÑ ƒêang load state dict...")
        model.load_state_dict(checkpoint['model'], strict=False)
        model.to(device)
        model.eval()
        
        logger.info(f"üîÑ ƒêang load tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, use_fast=False)
        
        # Load label mapping
        id2label = args.id2label
        
        logger.info(f"‚úÖ ƒê√£ load model th√†nh c√¥ng tr√™n {device}")
        logger.info(f"‚úÖ S·ªë l∆∞·ª£ng labels: {len(id2label)}")
        return True
        
    except FileNotFoundError as e:
        logger.error(f"‚ùå FILE NOT FOUND ERROR trong {current_file}:")
        logger.error(f"   File: {str(e)}")
        return False
    except torch.serialization.pickle.UnpicklingError as e:
        logger.error(f"‚ùå MODEL LOADING ERROR trong {current_file}:")
        logger.error(f"   L·ªói unpickling: {str(e)}")
        logger.error(f"   File model c√≥ th·ªÉ b·ªã corrupt: {model_path}")
        return False
    except Exception as e:
        logger.error(f"‚ùå UNEXPECTED ERROR trong {current_file} - load_model():")
        logger.error(f"   Chi ti·∫øt: {str(e)}")
        logger.error(f"   Traceback:")
        traceback.print_exc()
        return False

def predict_ner(sentence):
    """D·ª± ƒëo√°n NER cho m·ªôt c√¢u"""
    if not load_model():
        logger.error(f"‚ùå Kh√¥ng th·ªÉ load model trong {current_file}")
        return [], []
    
    if not sentence or not sentence.strip():
        logger.warning(f"‚ö†Ô∏è Input r·ªóng trong {current_file} - predict_ner()")
        return [], []
    
    try:
        logger.debug(f"üîÑ ƒêang predict cho: {sentence[:50]}...")
        
        # Normalize text
        sentence = normalize_text(sentence)
        
        # ‚úÖ FIX: Tokenize v·ªõi max_length ph√π h·ª£p
        encoding = tokenizer.encode_plus(
            sentence,
            return_tensors='pt',
            truncation=True,
            max_length=256,  # ‚úÖ Gi·∫£m t·ª´ 512 xu·ªëng 256
            padding=True,  # ‚úÖ Th√™m padding c·ªë ƒë·ªãnh
            is_split_into_words=False
        )
        
        input_ids = encoding['input_ids'].to(device)
        attention_mask = encoding['attention_mask'].to(device)
        
        # ‚úÖ FIX: T·∫°o valid_ids v√† label_masks ƒë√∫ng c√°ch
        batch_size, seq_len = input_ids.shape
        valid_ids = torch.ones((batch_size, seq_len), dtype=torch.long).to(device)
        label_masks = attention_mask.clone().to(device)
        
        # ‚úÖ DEBUG: Log k√≠ch th∆∞·ªõc tensor
        logger.debug(f"Input shape: {input_ids.shape}")
        logger.debug(f"Attention mask shape: {attention_mask.shape}")
        logger.debug(f"Valid ids shape: {valid_ids.shape}")
        logger.debug(f"Label masks shape: {label_masks.shape}")
        
        # Predict
        with torch.no_grad():
            output = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                valid_ids=valid_ids,
                label_masks=label_masks
            )
            
            if not hasattr(output, 'tags'):
                logger.error(f"‚ùå MODEL OUTPUT ERROR trong {current_file}:")
                logger.error(f"   Output kh√¥ng c√≥ 'tags' attribute")
                logger.error(f"   C√°c attributes: {dir(output)}")
                return [], []
            
            tag_ids = output.tags
        
        # ‚úÖ FIX: Ch·ªâ l·∫•y tokens kh√¥ng ph·∫£i special tokens
        tokens = tokenizer.convert_ids_to_tokens(input_ids[0])
        
        # ‚úÖ FIX: Ki·ªÉm tra length mismatch
        if len(tag_ids) != len(tokens):
            logger.error(f"‚ùå LENGTH MISMATCH ERROR trong {current_file}:")
            logger.error(f"   Tokens length: {len(tokens)}")
            logger.error(f"   Tag_ids length: {len(tag_ids)}")
            # C·∫Øt cho b·∫±ng nhau
            min_len = min(len(tokens), len(tag_ids))
            tokens = tokens[:min_len]
            tag_ids = tag_ids[:min_len]
        
        # ‚úÖ FIX: Ki·ªÉm tra tag_ids c√≥ n·∫±m trong ph·∫°m vi kh√¥ng
        valid_tag_ids = []
        for tag_id in tag_ids:
            if tag_id in id2label:
                valid_tag_ids.append(tag_id)
            else:
                logger.warning(f"‚ö†Ô∏è Tag ID {tag_id} kh√¥ng c√≥ trong id2label")
                valid_tag_ids.append(0)  # M·∫∑c ƒë·ªãnh l√† 'O'
        
        labels = [id2label[tag] for tag in valid_tag_ids]
        
        logger.debug(f"‚úÖ Predict th√†nh c√¥ng: {len(tokens)} tokens")
        return tokens, labels
        
    except KeyError as e:
        logger.error(f"‚ùå LABEL MAPPING ERROR trong {current_file} - predict_ner():")
        logger.error(f"   Kh√¥ng t√¨m th·∫•y label ID: {str(e)}")
        logger.error(f"   Available label IDs: {list(id2label.keys())}")
        return [], []
    except RuntimeError as e:
        logger.error(f"‚ùå TORCH/CUDA ERROR trong {current_file} - predict_ner():")
        logger.error(f"   Chi ti·∫øt: {str(e)}")
        return [], []
    except Exception as e:
        logger.error(f"‚ùå UNEXPECTED ERROR trong {current_file} - predict_ner():")
        logger.error(f"   Chi ti·∫øt: {str(e)}")
        logger.error(f"   Traceback:")
        traceback.print_exc()
        return [], []
    
def extract_entities(tokens, labels):
    """Tr√≠ch xu·∫•t entities t·ª´ tokens v√† labels"""
    if not tokens or not labels:
        logger.warning(f"‚ö†Ô∏è Input r·ªóng trong {current_file} - extract_entities()")
        return []
    
    if len(tokens) != len(labels):
        logger.error(f"‚ùå LENGTH MISMATCH trong {current_file} - extract_entities():")
        logger.error(f"   Tokens length: {len(tokens)}")
        logger.error(f"   Labels length: {len(labels)}")
        return []
    
    try:
        entities = []
        current_entity = ''
        current_label = None
        inside_entity = False
        
        for idx, (token, label) in enumerate(zip(tokens, labels)):
            # Skip special tokens
            if token in ['<s>', '</s>', '<pad>', '[PAD]', '[CLS]', '[SEP]']:
                continue
            
            # Clean token
            clean_token = token.replace('@@', '')
            
            if label.startswith('B-'):
                # L∆∞u entity tr∆∞·ªõc ƒë√≥ n·∫øu c√≥
                if current_entity and current_entity.strip():
                    # Ch·ªâ l∆∞u entity c√≥ √≠t nh·∫•t 2 k√Ω t·ª±
                    if len(current_entity.strip()) >= 2:
                        entities.append((current_entity.strip(), current_label))
                
                # B·∫Øt ƒë·∫ßu entity m·ªõi
                current_entity = clean_token
                current_label = label[2:]  # B·ªè 'B-'
                inside_entity = True
                
            elif label.startswith('I-') and inside_entity and current_label == label[2:]:
                # Ti·∫øp t·ª•c entity hi·ªán t·∫°i
                if token.startswith('@@'):
                    current_entity += clean_token
                else:
                    current_entity += ' ' + clean_token
                    
            else:
                # K·∫øt th√∫c entity
                if current_entity and current_entity.strip():
                    if len(current_entity.strip()) >= 2:
                        entities.append((current_entity.strip(), current_label))
                
                current_entity = ''
                inside_entity = False
                current_label = None
        
        # L∆∞u entity cu·ªëi c√πng n·∫øu c√≥
        if current_entity and current_entity.strip():
            if len(current_entity.strip()) >= 2:
                entities.append((current_entity.strip(), current_label))
        
        # Lo·∫°i b·ªè duplicate v√† sort
        unique_entities = list(set(entities))
        
        logger.debug(f"‚úÖ Tr√≠ch xu·∫•t ƒë∆∞·ª£c {len(unique_entities)} entities")
        return unique_entities
        
    except Exception as e:
        logger.error(f"‚ùå UNEXPECTED ERROR trong {current_file} - extract_entities():")
        logger.error(f"   Chi ti·∫øt: {str(e)}")
        logger.error(f"   Traceback:")
        traceback.print_exc()
        return []

def test_ner(text):
    """Test function ƒë·ªÉ ki·ªÉm tra NER"""
    print(f"üîÑ Testing trong file: {current_file}")
    print(f"Input: {text}")
    print("-" * 50)
    
    try:
        tokens, labels = predict_ner(text)
        
        if not tokens or not labels:
            print("‚ùå Kh√¥ng th·ªÉ predict NER")
            return
        
        print("Tokens v√† Labels:")
        for token, label in zip(tokens, labels):
            if token not in ['<s>', '</s>', '<pad>', '[PAD]']:
                print(f"{token:15} -> {label}")
        
        print("\nEntities:")
        entities = extract_entities(tokens, labels)
        if entities:
            for entity, label in entities:
                print(f"{entity:20} -> {label}")
        else:
            print("Kh√¥ng t√¨m th·∫•y entities")
        
        print("=" * 50)
        
    except Exception as e:
        logger.error(f"‚ùå ERROR trong {current_file} - test_ner():")
        logger.error(f"   Chi ti·∫øt: {str(e)}")
        traceback.print_exc()

# Test n·∫øu ch·∫°y tr·ª±c ti·∫øp
if __name__ == "__main__":
    try:
        logger.info(f"üöÄ B·∫Øt ƒë·∫ßu test NER trong file: {current_file}")
        
        # Test cases
        test_texts = [
            "Nguy·ªÖn VƒÉn A l√† sinh vi√™n t·∫°i ƒê·∫°i h·ªçc B√°ch khoa H√† N·ªôi",
            "C√¥ng ty Apple c√≥ tr·ª• s·ªü t·∫°i California, Hoa K·ª≥",
            "Th√†nh ph·ªë H·ªì Ch√≠ Minh l√† th√†nh ph·ªë l·ªõn nh·∫•t Vi·ªát Nam"
        ]
        
        for i, text in enumerate(test_texts, 1):
            logger.info(f"üìù Test case {i}/{len(test_texts)}")
            test_ner(text)
            
        logger.info(f"‚úÖ Ho√†n th√†nh test trong file: {current_file}")
        
    except Exception as e:
        logger.error(f"‚ùå MAIN ERROR trong {current_file}:")
        logger.error(f"   Chi ti·∫øt: {str(e)}")
        traceback.print_exc()