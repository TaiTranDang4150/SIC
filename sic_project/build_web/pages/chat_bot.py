import os
import json
import functools
import shutil
from typing import List, Dict, Any
from dotenv import load_dotenv

# LangChain split vÄƒn báº£n
from langchain.text_splitter import RecursiveCharacterTextSplitter

# CÃ¡c module Ä‘Ã£ tÃ¡ch riÃªng
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import Chroma

# Schema cá»§a LangChain
from langchain.schema import Document

# Giao diá»‡n
import streamlit as st
from pathlib import Path

# Load data tá»« cÆ¡ sá»Ÿ dá»¯ liá»‡u MongoDB
from utils.load_data import load_news_data # Keep this import

class RAGChatbot:
    """
    Lá»›p RAG Chatbot Ä‘Æ°á»£c tá»‘i Æ°u hÃ³a cho dá»¯ liá»‡u tin tá»©c.
    """
    def __init__(self, db_name: str = "vector_db_optimized"): # Removed data_path from here
        """
        Khá»Ÿi táº¡o RAG Chatbot.

        Args:
            db_name (str): TÃªn thÆ° má»¥c lÆ°u vector database.
        """
        load_dotenv(override=True)
        self.openai_api_key = os.getenv('OPENAI_API_KEY')
        if not self.openai_api_key:
            raise ValueError("OPENAI_API_KEY khÃ´ng Ä‘Æ°á»£c tÃ¬m tháº¥y trong biáº¿n mÃ´i trÆ°á»ng.")

        # self.data_path = data_path # No longer needed here
        self.db_name = db_name

        # --- Cáº¢I TIáº¾N: Cáº¥u hÃ¬nh Embedding vÃ  LLM tá»‘i Æ°u ---
        self.embedding_model = OpenAIEmbeddings(
            model="text-embedding-3-small",
            api_key=self.openai_api_key,
            chunk_size=1000,
            max_retries=3,
            request_timeout=60
        )
        self.llm_model = ChatOpenAI(
            model="gpt-4o-mini",
            temperature=0.2,  # Giáº£m nhiá»‡t Ä‘á»™ Ä‘á»ƒ cÃ¢u tráº£ lá»i bÃ¡m sÃ¡t sá»± tháº­t
            api_key=self.openai_api_key
        )
        self.vectorstore = None

    def check_and_fix_embedding_dimension(self):
        """
        Kiá»ƒm tra vÃ  xá»­ lÃ½ lá»—i khÃ´ng tÆ°Æ¡ng thÃ­ch dimension cá»§a embedding.
        Tá»± Ä‘á»™ng Ä‘á» xuáº¥t táº¡o láº¡i database náº¿u cÃ³ lá»—i.
        """
        if os.path.exists(self.db_name):
            try:
                st.info("Äang kiá»ƒm tra vector database hiá»‡n cÃ³...")
                temp_vectorstore = Chroma(
                    persist_directory=self.db_name,
                    embedding_function=self.embedding_model
                )
                temp_vectorstore.similarity_search("test", k=1)
                self.vectorstore = temp_vectorstore
                st.success("âœ… Sá»­ dá»¥ng vector database hiá»‡n cÃ³ thÃ nh cÃ´ng!")
                return True
            except Exception as e:
                error_msg = str(e)
                if "dimension" in error_msg.lower():
                    st.warning(f"âš ï¸ PhÃ¡t hiá»‡n lá»—i khÃ´ng tÆ°Æ¡ng thÃ­ch dimension: {error_msg}")
                    if st.button("XÃ³a vÃ  táº¡o láº¡i vector database"):
                        return self.rebuild_vectorstore()
                    else:
                        st.info("Nháº¥n nÃºt trÃªn Ä‘á»ƒ táº¡o láº¡i database vá»›i model embedding má»›i.")
                        return False
                else:
                    st.error(f"Lá»—i khÃ´ng xÃ¡c Ä‘á»‹nh khi táº£i database: {error_msg}")
                    return False
        else:
            st.info("Vector database chÆ°a tá»“n táº¡i. Báº¯t Ä‘áº§u quÃ¡ trÃ¬nh táº¡o má»›i.")
            return self.rebuild_vectorstore()

    def rebuild_vectorstore(self):
        """
        XÃ³a vÃ  xÃ¢y dá»±ng láº¡i toÃ n bá»™ vector database tá»« Ä‘áº§u.
        """
        try:
            if os.path.exists(self.db_name):
                shutil.rmtree(self.db_name)
                st.info("ÄÃ£ xÃ³a vector database cÅ©.")
            self.build_vectorstore()
            return True
        except Exception as e:
            st.error(f"Lá»—i nghiÃªm trá»ng khi táº¡o láº¡i vector database: {e}")
            return False

    # Removed the functools.lru_cache(maxsize=None) load_news_data method from here

    def create_documents_from_news(self, news_data: List[Dict[str, Any]]) -> List[Document]:
        """Táº¡o Ä‘á»‘i tÆ°á»£ng Document tá»« dá»¯ liá»‡u tin tá»©c vá»›i metadata chi tiáº¿t."""
        documents = []
        for item in news_data:
            content = item.get('content', '')
            if not content:
                continue

            metadata = {
                "id": str(item.get('id', '')),
                "title": item.get('title', 'N/A'),
                "url": item.get('url', ''),
                "author": item.get('author', 'N/A'),
                "tags": ', '.join(item.get('tags', [])),
                "time_posted": item.get('time_posted', 'N/A'),
                # "source_file": os.path.basename(self.data_path) # No longer rely on data_path directly
            }
            doc = Document(page_content=content.strip(), metadata=metadata)
            documents.append(doc)
        return documents

    # --- Cáº¢I TIáº¾N: Chiáº¿n lÆ°á»£c chunking tá»‘i Æ°u hÆ¡n ---
    def chunk_documents(self, documents: List[Document]) -> List[Document]:
        """
        Chia nhá» documents thÃ nh cÃ¡c chunks vá»›i kÃ­ch thÆ°á»›c vÃ  overlap há»£p lÃ½.
        """
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=700,
            chunk_overlap=150,
            length_function=len,
            separators=["\n\n", "\n", ". ", "!", "?", " ", ""]
        )
        chunked_docs = text_splitter.split_documents(documents)

        # Lá»c bá» cÃ¡c chunk quÃ¡ ngáº¯n, thÆ°á»ng khÃ´ng chá»©a nhiá»u thÃ´ng tin há»¯u Ã­ch
        filtered_chunks = [doc for doc in chunked_docs if len(doc.page_content) > 50]

        st.write(f"ÄÃ£ chia {len(documents)} bÃ i bÃ¡o thÃ nh {len(filtered_chunks)} chunks.")
        return filtered_chunks

    def build_vectorstore(self) -> None:
        """XÃ¢y dá»±ng vector store tá»« dá»¯ liá»‡u, vá»›i batch processing vÃ  progress bar."""
        # Use the imported load_news_data function
        news_data = load_news_data() # Call the external function here
        if not news_data:
            raise ValueError("KhÃ´ng thá»ƒ táº£i dá»¯ liá»‡u tá»« cÆ¡ sá»Ÿ dá»¯ liá»‡u.")

        documents = self.create_documents_from_news(news_data)
        if not documents:
            raise ValueError("KhÃ´ng cÃ³ document há»£p lá»‡ Ä‘á»ƒ táº¡o vector store.")

        chunked_docs = self.chunk_documents(documents)
        if not chunked_docs:
            st.warning("KhÃ´ng cÃ³ chunk nÃ o Ä‘Æ°á»£c táº¡o. Vui lÃ²ng kiá»ƒm tra láº¡i dá»¯ liá»‡u Ä‘áº§u vÃ o.")
            return

        st.info(f"Báº¯t Ä‘áº§u táº¡o vector database tá»« {len(chunked_docs)} chunks...")
        batch_size = 50
        batches = [chunked_docs[i:i + batch_size] for i in range(0, len(chunked_docs), batch_size)]

        # Khá»Ÿi táº¡o vectorstore vá»›i batch Ä‘áº§u tiÃªn
        self.vectorstore = Chroma.from_documents(
            documents=batches[0],
            embedding=self.embedding_model,
            persist_directory=self.db_name
        )

        # ThÃªm cÃ¡c batch cÃ²n láº¡i vá»›i progress bar
        progress_bar = st.progress(1 / len(batches), text=f"Äang xá»­ lÃ½ batch 1/{len(batches)}")
        for i, batch in enumerate(batches[1:], 1):
            try:
                self.vectorstore.add_documents(batch)
                progress_bar.progress((i + 1) / len(batches), text=f"Äang xá»­ lÃ½ batch {i+1}/{len(batches)}")
            except Exception as e:
                st.warning(f"Lá»—i khi thÃªm batch {i+1}: {e}")
                continue

        self.vectorstore.persist()
        st.success("ğŸ‰ HoÃ n thÃ nh táº¡o vector database!")

    # --- Cáº¢I TIáº¾N: Sá»­ dá»¥ng truy váº¥n MMR Ä‘á»ƒ Ä‘a dáº¡ng hÃ³a káº¿t quáº£ ---
    def get_query_results(self, query: str, k: int = 5) -> List[Dict[str, Any]]:
        """
        Truy váº¥n RAG sá»­ dá»¥ng Maximal Marginal Relevance (MMR) Ä‘á»ƒ tÄƒng sá»± Ä‘a dáº¡ng.
        """
        if not self.vectorstore:
            if not self.check_and_fix_embedding_dimension():
                return []

        try:
            retriever = self.vectorstore.as_retriever(
                search_type="mmr",
                search_kwargs={'k': k, 'fetch_k': 20}
            )
            docs = retriever.get_relevant_documents(query)

            return [{
                "content": doc.page_content,
                "metadata": doc.metadata,
                "similarity_score": "N/A (MMR)"
            } for doc in docs]
        except Exception as e:
            st.error(f"Lá»—i khi truy váº¥n vector database: {e}")
            return []

    def create_context_from_results(self, query_results: List[Dict[str, Any]]) -> str:
        """Táº¡o context string tá»« káº¿t quáº£ truy váº¥n Ä‘á»ƒ Ä‘Æ°a vÃ o prompt."""
        if not query_results:
            return ""

        context_parts = []
        for i, result in enumerate(query_results, 1):
            metadata = result["metadata"]
            context_entry = (
                f"[Nguá»“n {i}]\n"
                f"TiÃªu Ä‘á»: {metadata.get('title', 'N/A')}\n"
                f"Ná»™i dung: {result['content']}\n"
                f"TÃ¡c giáº£: {metadata.get('author', 'N/A')}\n"
                f"Thá»i gian: {metadata.get('time_posted', 'N/A')}\n"
                f"---"
            )
            context_parts.append(context_entry)

        return "\n\n".join(context_parts)

    # --- Cáº¢I TIáº¾N: Prompt cháº·t cháº½ vÃ  rÃµ rÃ ng hÆ¡n ---
    def create_prompt_template(self, question: str, context: str) -> str:
        """Táº¡o prompt template hiá»‡u quáº£ cho LLM."""
        if context:
            return f"""Báº¡n lÃ  má»™t trá»£ lÃ½ AI phÃ¢n tÃ­ch tin tá»©c.
**CHá»ˆ Ä‘Æ°á»£c phÃ©p sá»­ dá»¥ng thÃ´ng tin tá»« cÃ¡c tÃ i liá»‡u Ä‘Æ°á»£c cung cáº¥p dÆ°á»›i Ä‘Ã¢y Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i.** KhÃ´ng Ä‘Æ°á»£c sá»­ dá»¥ng báº¥t ká»³ kiáº¿n thá»©c bÃªn ngoÃ i nÃ o. Náº¿u thÃ´ng tin khÃ´ng cÃ³ trong tÃ i liá»‡u, hÃ£y tráº£ lá»i ráº±ng "ThÃ´ng tin khÃ´ng cÃ³ trong tÃ i liá»‡u Ä‘Æ°á»£c cung cáº¥p.".

TÃ€I LIá»†U:
---
{context}
---

CÃ‚U Há»I: {question}

HÆ¯á»šNG DáºªN:
- Tráº£ lá»i báº±ng tiáº¿ng Viá»‡t, chi tiáº¿t, rÃµ rÃ ng.
- TrÃ­ch dáº«n nguá»“n (vÃ­ dá»¥: [Nguá»“n 1], [Nguá»“n 2]) trong cÃ¢u tráº£ lá»i khi sá»­ dá»¥ng thÃ´ng tin tá»« Ä‘Ã³.

TRáº¢ Lá»œI:"""
        else:
            return f"""Báº¡n lÃ  má»™t trá»£ lÃ½ AI thÃ´ng minh. Tráº£ lá»i cÃ¢u há»i sau báº±ng kiáº¿n thá»©c chung cá»§a báº¡n.

CÃ‚U Há»I: {question}

TRáº¢ Lá»œI:"""

    def ask_question(self, question: str) -> Dict[str, Any]:
        """Quy trÃ¬nh hoÃ n chá»‰nh Ä‘á»ƒ há»i vÃ  nháº­n cÃ¢u tráº£ lá»i tá»« chatbot."""
        try:
            query_results = self.get_query_results(question, k=5)
            context = self.create_context_from_results(query_results)
            prompt = self.create_prompt_template(question, context)

            response = self.llm_model.invoke(prompt)
            answer = response.content

            detailed_sources = [{
                "index": i + 1,
                "title": r["metadata"].get("title", "N/A"),
                "url": r["metadata"].get("url", ""),
            } for i, r in enumerate(query_results)]

            return {
                "answer": answer,
                "detailed_sources": detailed_sources,
                "has_rag_context": bool(query_results),
                "context_used": context
            }
        except Exception as e:
            st.error(f"Lá»—i khi xá»­ lÃ½ cÃ¢u há»i: {e}")
            return {
                "answer": "Xin lá»—i, Ä‘Ã£ cÃ³ lá»—i xáº£y ra. Vui lÃ²ng thá»­ láº¡i.",
                "detailed_sources": [],
                "has_rag_context": False,
                "context_used": ""
            }

# --- Cáº¥u hÃ¬nh Streamlit vÃ  Singleton Pattern ---
@st.cache_resource
def get_chatbot():
    """Táº¡o vÃ  cache singleton chatbot instance."""

    # --- Cáº¢I TIáº¾N: Sá»­ dá»¥ng Ä‘Æ°á»ng dáº«n tÆ°Æ¡ng Ä‘á»‘i ---
    # Äáº£m báº£o file data `ok_vnnet.json` náº±m trong thÆ° má»¥c con `data`
    # cá»§a thÆ° má»¥c chá»©a file python nÃ y.
    # Cáº¥u trÃºc thÆ° má»¥c:
    # /your_project_folder
    #   |- app.py (file nÃ y)
    #   |- /data
    #       |- ok_vnnet.json

    # current_dir = Path(__file__).parent # No longer needed for data path
    # DATA_PATH_STR = "C:\\Users\\Admin\\PyCharmMiscProject\\sic_project\\data\\ok_vnnet.json" # No longer needed
    DB_NAME = "vector_db_optimized"

    # DATA_PATH_OBJ = Path(DATA_PATH_STR) # No longer needed
    # if not DATA_PATH_OBJ.exists(): # No longer needed for file check
    #     st.error(f"KhÃ´ng tÃ¬m tháº¥y file dá»¯ liá»‡u táº¡i: {DATA_PATH_OBJ}")
    #     st.stop()

    chatbot = RAGChatbot(db_name=DB_NAME) # Pass only db_name
    return chatbot

def build_qa_chain():
    """Khá»Ÿi táº¡o há»‡ thá»‘ng RAG vÃ  tráº£ vá» chatbot instance."""
    chatbot = get_chatbot()
    if not chatbot.vectorstore:
        with st.spinner("Äang khá»Ÿi táº¡o há»‡ thá»‘ng RAG..."):
            chatbot.check_and_fix_embedding_dimension()
    return chatbot

# --- CÃ¡c hÃ m tiá»‡n Ã­ch ---
def ask_chatbot(question: str) -> Dict[str, Any]:
    """HÃ m tiá»‡n Ã­ch Ä‘á»ƒ Ä‘áº·t cÃ¢u há»i cho chatbot."""
    chatbot = get_chatbot()
    return chatbot.ask_question(question)

def get_query_results_debug(question: str, k: int = 5):
    """HÃ m tiá»‡n Ã­ch Ä‘á»ƒ debug, xem trá»±c tiáº¿p káº¿t quáº£ truy váº¥n RAG."""
    chatbot = get_chatbot()
    return chatbot.get_query_results(question, k)