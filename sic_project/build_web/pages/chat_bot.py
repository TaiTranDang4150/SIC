import os
import json
import functools
import shutil
from typing import List, Dict, Any
from dotenv import load_dotenv

# LangChain split vƒÉn b·∫£n
from langchain.text_splitter import RecursiveCharacterTextSplitter

# C√°c module ƒë√£ t√°ch ri√™ng
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import Chroma

# Schema c·ªßa LangChain
from langchain.schema import Document

# Giao di·ªán
import streamlit as st
from pathlib import Path

# Load data t·ª´ c∆° s·ªü d·ªØ li·ªáu MongoDB
from utils.load_data import load_news_data # Keep this import

class RAGChatbot:
    """
    L·ªõp RAG Chatbot ƒë∆∞·ª£c t·ªëi ∆∞u h√≥a cho d·ªØ li·ªáu tin t·ª©c.
    """
    def __init__(self, db_name: str = "vector_db_optimized"): # Removed data_path from here
        """
        Kh·ªüi t·∫°o RAG Chatbot.

        Args:
            db_name (str): T√™n th∆∞ m·ª•c l∆∞u vector database.
        """
        load_dotenv(override=True)
        self.openai_api_key = os.getenv('OPENAI_API_KEY')
        if not self.openai_api_key:
            raise ValueError("OPENAI_API_KEY kh√¥ng ƒë∆∞·ª£c t√¨m th·∫•y trong bi·∫øn m√¥i tr∆∞·ªùng.")

        # self.data_path = data_path # No longer needed here
        self.db_name = db_name

        # --- C·∫¢I TI·∫æN: C·∫•u h√¨nh Embedding v√† LLM t·ªëi ∆∞u ---
        self.embedding_model = OpenAIEmbeddings(
            model="text-embedding-3-small",
            api_key=self.openai_api_key,
            chunk_size=1000,
            max_retries=3,
            request_timeout=60
        )
        self.llm_model = ChatOpenAI(
            model="gpt-4o-mini",
            temperature=0.2,  # Gi·∫£m nhi·ªát ƒë·ªô ƒë·ªÉ c√¢u tr·∫£ l·ªùi b√°m s√°t s·ª± th·∫≠t
            api_key=self.openai_api_key
        )
        self.vectorstore = None

    def check_and_fix_embedding_dimension(self):
        """
        Ki·ªÉm tra v√† x·ª≠ l√Ω l·ªói kh√¥ng t∆∞∆°ng th√≠ch dimension c·ªßa embedding.
        T·ª± ƒë·ªông ƒë·ªÅ xu·∫•t t·∫°o l·∫°i database n·∫øu c√≥ l·ªói.
        """
        if os.path.exists(self.db_name):
            try:
                st.info("ƒêang ki·ªÉm tra vector database hi·ªán c√≥...")
                temp_vectorstore = Chroma(
                    persist_directory=self.db_name,
                    embedding_function=self.embedding_model
                )
                temp_vectorstore.similarity_search("test", k=1)
                self.vectorstore = temp_vectorstore
                st.success("‚úÖ S·ª≠ d·ª•ng vector database hi·ªán c√≥ th√†nh c√¥ng!")
                return True
            except Exception as e:
                error_msg = str(e)
                if "dimension" in error_msg.lower():
                    st.warning(f"‚ö†Ô∏è Ph√°t hi·ªán l·ªói kh√¥ng t∆∞∆°ng th√≠ch dimension: {error_msg}")
                    if st.button("X√≥a v√† t·∫°o l·∫°i vector database"):
                        return self.rebuild_vectorstore()
                    else:
                        st.info("Nh·∫•n n√∫t tr√™n ƒë·ªÉ t·∫°o l·∫°i database v·ªõi model embedding m·ªõi.")
                        return False
                else:
                    st.error(f"L·ªói kh√¥ng x√°c ƒë·ªãnh khi t·∫£i database: {error_msg}")
                    return False
        else:
            st.info("Vector database ch∆∞a t·ªìn t·∫°i. B·∫Øt ƒë·∫ßu qu√° tr√¨nh t·∫°o m·ªõi.")
            return self.rebuild_vectorstore()

    def rebuild_vectorstore(self):
        """
        X√≥a v√† x√¢y d·ª±ng l·∫°i to√†n b·ªô vector database t·ª´ ƒë·∫ßu.
        """
        try:
            if os.path.exists(self.db_name):
                shutil.rmtree(self.db_name)
                st.info("ƒê√£ x√≥a vector database c≈©.")
            self.build_vectorstore()
            return True
        except Exception as e:
            st.error(f"L·ªói nghi√™m tr·ªçng khi t·∫°o l·∫°i vector database: {e}")
            return False

    # Removed the functools.lru_cache(maxsize=None) load_news_data method from here

    def create_documents_from_news(self, news_data: List[Dict[str, Any]]) -> List[Document]:
        """T·∫°o ƒë·ªëi t∆∞·ª£ng Document t·ª´ d·ªØ li·ªáu tin t·ª©c v·ªõi metadata chi ti·∫øt."""
        documents = []
        for item in news_data:
            content = item.get('content', '')
            if not content:
                continue

            metadata = {
                "id": str(item.get('id', '')),
                "title": item.get('title', 'N/A'),
                "url": item.get('url', ''),
                "author": item.get('author', 'N/A'),
                "tags": ', '.join(item.get('tags', [])),
                "time_posted": item.get('time_posted', 'N/A'),
                # "source_file": os.path.basename(self.data_path) # No longer rely on data_path directly
            }
            doc = Document(page_content=content.strip(), metadata=metadata)
            documents.append(doc)
        return documents

    # --- C·∫¢I TI·∫æN: Chi·∫øn l∆∞·ª£c chunking t·ªëi ∆∞u h∆°n ---
    def chunk_documents(self, documents: List[Document]) -> List[Document]:
        """
        Chia nh·ªè documents th√†nh c√°c chunks v·ªõi k√≠ch th∆∞·ªõc v√† overlap h·ª£p l√Ω.
        """
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=700,
            chunk_overlap=150,
            length_function=len,
            separators=["\n\n", "\n", ". ", "!", "?", " ", ""]
        )
        chunked_docs = text_splitter.split_documents(documents)

        # L·ªçc b·ªè c√°c chunk qu√° ng·∫Øn, th∆∞·ªùng kh√¥ng ch·ª©a nhi·ªÅu th√¥ng tin h·ªØu √≠ch
        filtered_chunks = [doc for doc in chunked_docs if len(doc.page_content) > 50]

        st.write(f"ƒê√£ chia {len(documents)} b√†i b√°o th√†nh {len(filtered_chunks)} chunks.")
        return filtered_chunks

    def build_vectorstore(self) -> None:
        """X√¢y d·ª±ng vector store t·ª´ d·ªØ li·ªáu, v·ªõi batch processing v√† progress bar."""
        # Use the imported load_news_data function
        news_data = load_news_data() # Call the external function here
        if not news_data:
            raise ValueError("Kh√¥ng th·ªÉ t·∫£i d·ªØ li·ªáu t·ª´ c∆° s·ªü d·ªØ li·ªáu.")

        documents = self.create_documents_from_news(news_data)
        if not documents:
            raise ValueError("Kh√¥ng c√≥ document h·ª£p l·ªá ƒë·ªÉ t·∫°o vector store.")

        chunked_docs = self.chunk_documents(documents)
        if not chunked_docs:
            st.warning("Kh√¥ng c√≥ chunk n√†o ƒë∆∞·ª£c t·∫°o. Vui l√≤ng ki·ªÉm tra l·∫°i d·ªØ li·ªáu ƒë·∫ßu v√†o.")
            return

        st.info(f"B·∫Øt ƒë·∫ßu t·∫°o vector database t·ª´ {len(chunked_docs)} chunks...")
        batch_size = 50
        batches = [chunked_docs[i:i + batch_size] for i in range(0, len(chunked_docs), batch_size)]

        # Kh·ªüi t·∫°o vectorstore v·ªõi batch ƒë·∫ßu ti√™n
        self.vectorstore = Chroma.from_documents(
            documents=batches[0],
            embedding=self.embedding_model,
            persist_directory=self.db_name
        )

        # Th√™m c√°c batch c√≤n l·∫°i v·ªõi progress bar
        progress_bar = st.progress(1 / len(batches), text=f"ƒêang x·ª≠ l√Ω batch 1/{len(batches)}")
        for i, batch in enumerate(batches[1:], 1):
            try:
                self.vectorstore.add_documents(batch)
                progress_bar.progress((i + 1) / len(batches), text=f"ƒêang x·ª≠ l√Ω batch {i+1}/{len(batches)}")
            except Exception as e:
                st.warning(f"L·ªói khi th√™m batch {i+1}: {e}")
                continue

        self.vectorstore.persist()
        st.success("üéâ Ho√†n th√†nh t·∫°o vector database!")

    # --- C·∫¢I TI·∫æN: S·ª≠ d·ª•ng truy v·∫•n MMR ƒë·ªÉ ƒëa d·∫°ng h√≥a k·∫øt qu·∫£ ---
    def get_query_results(self, query: str, k: int = 5) -> List[Dict[str, Any]]:
        """
        Truy v·∫•n RAG s·ª≠ d·ª•ng Maximal Marginal Relevance (MMR) ƒë·ªÉ tƒÉng s·ª± ƒëa d·∫°ng.
        """
        if not self.vectorstore:
            if not self.check_and_fix_embedding_dimension():
                return []

        try:
            retriever = self.vectorstore.as_retriever(
                search_type="mmr",
                search_kwargs={'k': k, 'fetch_k': 20}
            )
            docs = retriever.get_relevant_documents(query)

            return [{
                "content": doc.page_content,
                "metadata": doc.metadata,
                "similarity_score": "N/A (MMR)"
            } for doc in docs]
        except Exception as e:
            st.error(f"L·ªói khi truy v·∫•n vector database: {e}")
            return []

    def create_context_from_results(self, query_results: List[Dict[str, Any]]) -> str:
        """T·∫°o context string t·ª´ k·∫øt qu·∫£ truy v·∫•n ƒë·ªÉ ƒë∆∞a v√†o prompt."""
        if not query_results:
            return ""

        context_parts = []
        for i, result in enumerate(query_results, 1):
            metadata = result["metadata"]
            context_entry = (
                f"[Ngu·ªìn {i}]\n"
                f"Ti√™u ƒë·ªÅ: {metadata.get('title', 'N/A')}\n"
                f"N·ªôi dung: {result['content']}\n"
                f"T√°c gi·∫£: {metadata.get('author', 'N/A')}\n"
                f"Th·ªùi gian: {metadata.get('time_posted', 'N/A')}\n"
                f"---"
            )
            context_parts.append(context_entry)

        return "\n\n".join(context_parts)

    # --- C·∫¢I TI·∫æN: Prompt ch·∫∑t ch·∫Ω v√† r√µ r√†ng h∆°n ---
    def create_prompt_template(self, question: str, context: str) -> str:
        """T·∫°o prompt template hi·ªáu qu·∫£ cho LLM."""
        if context:
            return f"""B·∫°n l√† m·ªôt tr·ª£ l√Ω AI ph√¢n t√≠ch tin t·ª©c.
**CH·ªà ƒë∆∞·ª£c ph√©p s·ª≠ d·ª•ng th√¥ng tin t·ª´ c√°c t√†i li·ªáu ƒë∆∞·ª£c cung c·∫•p d∆∞·ªõi ƒë√¢y ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi.** Kh√¥ng ƒë∆∞·ª£c s·ª≠ d·ª•ng b·∫•t k·ª≥ ki·∫øn th·ª©c b√™n ngo√†i n√†o. N·∫øu th√¥ng tin kh√¥ng c√≥ trong t√†i li·ªáu, h√£y tr·∫£ l·ªùi r·∫±ng "Th√¥ng tin kh√¥ng c√≥ trong t√†i li·ªáu ƒë∆∞·ª£c cung c·∫•p.".

T√ÄI LI·ªÜU:
---
{context}
---

C√ÇU H·ªéI: {question}

H∆Ø·ªöNG D·∫™N:
- Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát, chi ti·∫øt, r√µ r√†ng.
- Tr√≠ch d·∫´n ngu·ªìn (v√≠ d·ª•: [Ngu·ªìn 1], [Ngu·ªìn 2]) trong c√¢u tr·∫£ l·ªùi khi s·ª≠ d·ª•ng th√¥ng tin t·ª´ ƒë√≥.

TR·∫¢ L·ªúI:"""
        else:
            return f"""B·∫°n l√† m·ªôt tr·ª£ l√Ω AI th√¥ng minh. Tr·∫£ l·ªùi c√¢u h·ªèi sau b·∫±ng ki·∫øn th·ª©c chung c·ªßa b·∫°n.

C√ÇU H·ªéI: {question}

TR·∫¢ L·ªúI:"""

    def ask_question(self, question: str) -> Dict[str, Any]:
        """Quy tr√¨nh ho√†n ch·ªânh ƒë·ªÉ h·ªèi v√† nh·∫≠n c√¢u tr·∫£ l·ªùi t·ª´ chatbot."""
        try:
            query_results = self.get_query_results(question, k=5)
            context = self.create_context_from_results(query_results)
            prompt = self.create_prompt_template(question, context)

            response = self.llm_model.invoke(prompt)
            answer = response.content

            detailed_sources = [{
                "index": i + 1,
                "title": r["metadata"].get("title", "N/A"),
                "url": r["metadata"].get("url", ""),
            } for i, r in enumerate(query_results)]

            return {
                "answer": answer,
                "detailed_sources": detailed_sources,
                "has_rag_context": bool(query_results),
                "context_used": context
            }
        except Exception as e:
            st.error(f"L·ªói khi x·ª≠ l√Ω c√¢u h·ªèi: {e}")
            return {
                "answer": "Xin l·ªói, ƒë√£ c√≥ l·ªói x·∫£y ra. Vui l√≤ng th·ª≠ l·∫°i.",
                "detailed_sources": [],
                "has_rag_context": False,
                "context_used": ""
            }

# --- C·∫•u h√¨nh Streamlit v√† Singleton Pattern ---
@st.cache_resource
def get_chatbot():
    """T·∫°o v√† cache singleton chatbot instance."""

    # --- C·∫¢I TI·∫æN: S·ª≠ d·ª•ng ƒë∆∞·ªùng d·∫´n t∆∞∆°ng ƒë·ªëi ---
    # ƒê·∫£m b·∫£o file data `ok_vnnet.json` n·∫±m trong th∆∞ m·ª•c con `data`
    # c·ªßa th∆∞ m·ª•c ch·ª©a file python n√†y.
    # C·∫•u tr√∫c th∆∞ m·ª•c:
    # /your_project_folder
    #   |- app.py (file n√†y)
    #   |- /data
    #       |- ok_vnnet.json

    # current_dir = Path(__file__).parent # No longer needed for data path
    # DATA_PATH_STR = "C:\\Users\\Admin\\PyCharmMiscProject\\sic_project\\data\\ok_vnnet.json" # No longer needed
    DB_NAME = "vector_db_optimized"

    # DATA_PATH_OBJ = Path(DATA_PATH_STR) # No longer needed
    # if not DATA_PATH_OBJ.exists(): # No longer needed for file check
    #     st.error(f"Kh√¥ng t√¨m th·∫•y file d·ªØ li·ªáu t·∫°i: {DATA_PATH_OBJ}")
    #     st.stop()

    chatbot = RAGChatbot(db_name=DB_NAME) # Pass only db_name
    return chatbot

def build_qa_chain():
    """Kh·ªüi t·∫°o h·ªá th·ªëng RAG v√† tr·∫£ v·ªÅ chatbot instance."""
    chatbot = get_chatbot()
    if not chatbot.vectorstore:
        with st.spinner("ƒêang kh·ªüi t·∫°o h·ªá th·ªëng RAG..."):
            chatbot.check_and_fix_embedding_dimension()
    return chatbot

# --- C√°c h√†m ti·ªán √≠ch ---
def ask_chatbot(question: str) -> Dict[str, Any]:
    """H√†m ti·ªán √≠ch ƒë·ªÉ ƒë·∫∑t c√¢u h·ªèi cho chatbot."""
    chatbot = get_chatbot()
    return chatbot.ask_question(question)

def get_query_results_debug(question: str, k: int = 5):
    """H√†m ti·ªán √≠ch ƒë·ªÉ debug, xem tr·ª±c ti·∫øp k·∫øt qu·∫£ truy v·∫•n RAG."""
    chatbot = get_chatbot()
    return chatbot.get_query_results(question, k)