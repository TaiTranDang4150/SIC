import os
import json
import functools
import shutil
from typing import List, Dict, Any
from dotenv import load_dotenv

# --- Cáº¢I TIáº¾N: ThÃªm cÃ¡c import cáº§n thiáº¿t cho viá»‡c tinh chá»‰nh ---
# LangChain split vÄƒn báº£n theo ngá»¯ nghÄ©a
from langchain_experimental.text_splitter import SemanticChunker
# LangChain Ä‘á»ƒ káº¿t há»£p nhiá»u retriever
from langchain.retrievers import EnsembleRetriever

# CÃ¡c module Ä‘Ã£ tÃ¡ch riÃªng
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import Chroma

# Schema cá»§a LangChain
from langchain.schema import Document

# Giao diá»‡n
import streamlit as st
from pathlib import Path

# Load data tá»« cÆ¡ sá»Ÿ dá»¯ liá»‡u MongoDB
from utils.load_data import load_news_data  # Keep this import


class RAGChatbot:
    """
    Lá»›p RAG Chatbot Ä‘Æ°á»£c tá»‘i Æ°u hÃ³a cho dá»¯ liá»‡u tin tá»©c.
    """

    def __init__(self, db_name: str = "vector_db_optimized_v2"):  # Äá»•i tÃªn DB Ä‘á»ƒ trÃ¡nh xung Ä‘á»™t vá»›i DB cÅ©
        """
        Khá»Ÿi táº¡o RAG Chatbot.

        Args:
            db_name (str): TÃªn thÆ° má»¥c lÆ°u vector database.
        """
        load_dotenv(override=True)
        self.openai_api_key = os.getenv('OPENAI_API_KEY')
        if not self.openai_api_key:
            raise ValueError("OPENAI_API_KEY khÃ´ng Ä‘Æ°á»£c tÃ¬m tháº¥y trong biáº¿n mÃ´i trÆ°á»ng.")

        self.db_name = db_name

        # --- Cáº¥u hÃ¬nh Embedding vÃ  LLM váº«n giá»¯ nguyÃªn tá»‘i Æ°u cÅ© ---
        self.embedding_model = OpenAIEmbeddings(
            model="text-embedding-3-small",
            api_key=self.openai_api_key,
            chunk_size=1000,
            max_retries=3,
            request_timeout=60
        )
        self.llm_model = ChatOpenAI(
            model="gpt-4o-mini",
            temperature=0.15,  # Giáº£m nháº¹ nhiá»‡t Ä‘á»™ hÆ¡n ná»¯a Ä‘á»ƒ cÃ¢u tráº£ lá»i bÃ¡m sÃ¡t sá»± tháº­t
            api_key=self.openai_api_key
        )
        self.vectorstore = None

    def check_and_fix_embedding_dimension(self):
        """
        Kiá»ƒm tra vÃ  xá»­ lÃ½ lá»—i khÃ´ng tÆ°Æ¡ng thÃ­ch dimension cá»§a embedding.
        Tá»± Ä‘á»™ng Ä‘á» xuáº¥t táº¡o láº¡i database náº¿u cÃ³ lá»—i.
        """
        if os.path.exists(self.db_name):
            try:
                st.info("Äang kiá»ƒm tra vector database hiá»‡n cÃ³...")
                temp_vectorstore = Chroma(
                    persist_directory=self.db_name,
                    embedding_function=self.embedding_model
                )
                temp_vectorstore.similarity_search("test", k=1)
                self.vectorstore = temp_vectorstore
                st.success("âœ… Sá»­ dá»¥ng vector database hiá»‡n cÃ³ thÃ nh cÃ´ng!")
                return True
            except Exception as e:
                error_msg = str(e)
                # TÄƒng cÆ°á»ng kháº£ nÄƒng báº¯t lá»—i dimension
                if "dimension" in error_msg.lower() or "dimensionality" in error_msg.lower():
                    st.warning(f"âš ï¸ PhÃ¡t hiá»‡n lá»—i khÃ´ng tÆ°Æ¡ng thÃ­ch dimension: {error_msg}")
                    if st.button("XÃ³a vÃ  táº¡o láº¡i vector database vá»›i cáº¥u hÃ¬nh má»›i"):
                        return self.rebuild_vectorstore()
                    else:
                        st.info("Nháº¥n nÃºt trÃªn Ä‘á»ƒ táº¡o láº¡i database vá»›i model embedding má»›i.")
                        return False
                else:
                    st.error(f"Lá»—i khÃ´ng xÃ¡c Ä‘á»‹nh khi táº£i database: {error_msg}")
                    return False
        else:
            st.info("Vector database chÆ°a tá»“n táº¡i. Báº¯t Ä‘áº§u quÃ¡ trÃ¬nh táº¡o má»›i.")
            return self.rebuild_vectorstore()

    def rebuild_vectorstore(self):
        """
        XÃ³a vÃ  xÃ¢y dá»±ng láº¡i toÃ n bá»™ vector database tá»« Ä‘áº§u.
        """
        try:
            if os.path.exists(self.db_name):
                shutil.rmtree(self.db_name)
                st.info("ÄÃ£ xÃ³a vector database cÅ©.")
            self.build_vectorstore()
            return True
        except Exception as e:
            st.error(f"Lá»—i nghiÃªm trá»ng khi táº¡o láº¡i vector database: {e}")
            return False

    def create_documents_from_news(self, news_data: List[Dict[str, Any]]) -> List[Document]:
        documents = []
        for item in news_data:
            # --- KIá»‚M TRA VÃ€ Ã‰P KIá»‚U CONTENT ---
            content_raw = item.get('content', '')
            if not isinstance(content_raw, str):
                # Náº¿u content khÃ´ng pháº£i lÃ  chuá»—i, cá»‘ gáº¯ng chuyá»ƒn nÃ³ thÃ nh chuá»—i
                try:
                    content = str(content_raw)
                except Exception as e:
                    st.warning(
                        f"KhÃ´ng thá»ƒ chuyá»ƒn Ä‘á»•i ná»™i dung thÃ nh chuá»—i cho ID {item.get('id', 'N/A')}: {content_raw}. Bá» qua báº£n ghi nÃ y. Lá»—i: {e}")
                    continue  # Bá» qua báº£n ghi nÃ y náº¿u khÃ´ng thá»ƒ chuyá»ƒn Ä‘á»•i
            else:
                content = content_raw

            if not content:  # Kiá»ƒm tra láº¡i sau khi Ã©p kiá»ƒu
                continue

            # ... (Pháº§n xá»­ lÃ½ tags Ä‘Ã£ Ä‘Æ°á»£c sá»­a á»Ÿ láº§n trÆ°á»›c, giá»¯ nguyÃªn) ...
            tags_value = item.get('tags')
            tags_str = "N/A"

            if isinstance(tags_value, list):
                tags_str = ', '.join(str(tag) for tag in tags_value if tag is not None)
            elif isinstance(tags_value, (str, float, int)):
                tags_str = str(tags_value)

            metadata = {
                "id": str(item.get('id', '')),
                "title": item.get('title', 'N/A'),
                "url": item.get('url', ''),
                "author": item.get('author', 'N/A'),
                "tags": tags_str,
                "time_posted": item.get('time_posted', 'N/A'),
            }
            # Gá»i .strip() chá»‰ khi content Ä‘Ã£ cháº¯c cháº¯n lÃ  chuá»—i
            doc = Document(page_content=content.strip(), metadata=metadata)
            documents.append(doc)
        return documents

    # --- Cáº¢I TIáº¾N #1: Sá»­ dá»¥ng Semantic Chunking ---
    # ChÃº thÃ­ch: Thay vÃ¬ chia vÄƒn báº£n theo sá»‘ lÆ°á»£ng kÃ½ tá»± má»™t cÃ¡ch mÃ¡y mÃ³c,
    # SemanticChunker sá»­ dá»¥ng mÃ´ hÃ¬nh embedding Ä‘á»ƒ tÃ¬m ra nhá»¯ng Ä‘iá»ƒm ngáº¯t tá»± nhiÃªn
    # trong vÄƒn báº£n, nÆ¡i ngá»¯ nghÄ©a thay Ä‘á»•i. Äiá»u nÃ y giÃºp má»—i chunk chá»©a Ä‘á»±ng
    # má»™t Ã½ tÆ°á»Ÿng hoáº·c má»™t chá»§ Ä‘á» trá»n váº¹n hÆ¡n, cung cáº¥p context cháº¥t lÆ°á»£ng cao hÆ¡n cho LLM.
    def chunk_documents(self, documents: List[Document]) -> List[Document]:
        """
        Chia nhá» documents báº±ng Semantic Chunker Ä‘á»ƒ giá»¯ trá»n váº¹n ngá»¯ nghÄ©a.
        """
        st.info("Báº¯t Ä‘áº§u chia nhá» vÄƒn báº£n theo ngá»¯ nghÄ©a (Semantic Chunking)...")
        # Khá»Ÿi táº¡o SemanticChunker vá»›i chÃ­nh embedding model Ä‘ang dÃ¹ng
        text_splitter = SemanticChunker(
            self.embedding_model,
            breakpoint_threshold_type="percentile"
            # NgÆ°á»¡ng Ä‘á»ƒ quyáº¿t Ä‘á»‹nh Ä‘iá»ƒm ngáº¯t, 'percentile' thÆ°á»ng cho káº¿t quáº£ tá»‘t.
        )

        all_chunks = []
        progress_bar = st.progress(0, text="Äang xá»­ lÃ½ vÄƒn báº£n...")
        for i, doc in enumerate(documents):
            # SemanticChunker cáº§n cháº¡y trÃªn tá»«ng document má»™t
            chunks = text_splitter.create_documents([doc.page_content])
            # GÃ¡n láº¡i metadata tá»« document gá»‘c cho cÃ¡c chunk má»›i Ä‘Æ°á»£c táº¡o
            for chunk in chunks:
                chunk.metadata = doc.metadata.copy()
            all_chunks.extend(chunks)
            progress_bar.progress((i + 1) / len(documents), text=f"ÄÃ£ xá»­ lÃ½ {i + 1}/{len(documents)} bÃ i bÃ¡o")

        st.success(f"ÄÃ£ chia {len(documents)} bÃ i bÃ¡o thÃ nh {len(all_chunks)} chunks theo ngá»¯ nghÄ©a.")
        return all_chunks

    def build_vectorstore(self) -> None:
        """XÃ¢y dá»±ng vector store tá»« dá»¯ liá»‡u, vá»›i batch processing vÃ  progress bar."""
        news_data_full = load_news_data()
        if not news_data_full:
            raise ValueError("KhÃ´ng thá»ƒ táº£i dá»¯ liá»‡u tá»« cÆ¡ sá»Ÿ dá»¯ liá»‡u.")

        # Giá»›i háº¡n sá»‘ lÆ°á»£ng bÃ i bÃ¡o Ä‘á»ƒ chunk (100 bÃ i)
        limited_news_data = news_data_full[:100]
        st.info(f"Äang xá»­ lÃ½ {len(limited_news_data)} bÃ i bÃ¡o Ä‘á»ƒ táº¡o vector database...")

        documents = self.create_documents_from_news(limited_news_data)
        if not documents:
            raise ValueError("KhÃ´ng cÃ³ document há»£p lá»‡ Ä‘á»ƒ táº¡o vector store.")

        chunked_docs = self.chunk_documents(documents)
        if not chunked_docs:
            st.warning("KhÃ´ng cÃ³ chunk nÃ o Ä‘Æ°á»£c táº¡o. Vui lÃ²ng kiá»ƒm tra láº¡i dá»¯ liá»‡u Ä‘áº§u vÃ o.")
            return

        st.info(f"Báº¯t Ä‘áº§u táº¡o vector database tá»« {len(chunked_docs)} chunks...")
        batch_size = 50
        batches = [chunked_docs[i:i + batch_size] for i in range(0, len(chunked_docs), batch_size)]

        if not batches:
            st.warning("KhÃ´ng cÃ³ batch nÃ o Ä‘á»ƒ xá»­ lÃ½.")
            return

        self.vectorstore = Chroma.from_documents(
            documents=batches[0],
            embedding=self.embedding_model,
            persist_directory=self.db_name
        )

        progress_bar = st.progress(1 / len(batches), text=f"Äang xá»­ lÃ½ batch 1/{len(batches)}")
        for i, batch in enumerate(batches[1:], 1):
            try:
                self.vectorstore.add_documents(batch)
                progress_bar.progress((i + 1) / len(batches), text=f"Äang xá»­ lÃ½ batch {i + 1}/{len(batches)}")
            except Exception as e:
                st.warning(f"Lá»—i khi thÃªm batch {i + 1}: {e}")
                continue

        self.vectorstore.persist()
        st.success("ğŸ‰ HoÃ n thÃ nh táº¡o vector database!")

    # --- Cáº¢I TIáº¾N #2: Sá»­ dá»¥ng Ensemble Retriever ---
    def get_query_results(self, query: str, k: int = 6) -> List[Dict[str, Any]]:
        """
        Truy váº¥n RAG sá»­ dá»¥ng Ensemble Retriever Ä‘á»ƒ káº¿t há»£p Ä‘iá»ƒm máº¡nh cá»§a
        similarity search vÃ  MMR.
        """
        if not self.vectorstore:
            if not self.check_and_fix_embedding_dimension():
                return []

        try:
            similarity_retriever = self.vectorstore.as_retriever(
                search_type="similarity",
                search_kwargs={'k': k}
            )
            mmr_retriever = self.vectorstore.as_retriever(
                search_type="mmr",
                search_kwargs={'k': k, 'fetch_k': 20}
            )

            ensemble_retriever = EnsembleRetriever(
                retrievers=[similarity_retriever, mmr_retriever],
                weights=[0.5, 0.5]
            )

            docs = ensemble_retriever.get_relevant_documents(query)
            unique_docs = {doc.page_content: doc for doc in docs}.values()

            return [{
                "content": doc.page_content,
                "metadata": doc.metadata,
                "similarity_score": "N/A (Ensemble)"  # CÃ³ thá»ƒ bá»• sung logic tÃ­nh toÃ¡n Ä‘iá»ƒm sá»‘ tá»•ng há»£p náº¿u muá»‘n
            } for doc in unique_docs]
        except Exception as e:
            st.error(f"Lá»—i khi truy váº¥n vector database: {e}")
            return []

    def create_context_from_results(self, query_results: List[Dict[str, Any]]) -> str:
        """Táº¡o context string tá»« káº¿t quáº£ truy váº¥n Ä‘á»ƒ Ä‘Æ°a vÃ o prompt."""
        if not query_results:
            return ""

        context_parts = []
        for i, result in enumerate(query_results, 1):
            metadata = result["metadata"]
            context_entry = (
                f"[Nguá»“n {i}]\n"
                f"TiÃªu Ä‘á»: {metadata.get('title', 'N/A')}\n"
                f"Ná»™i dung: {result['content']}\n"
                f"TÃ¡c giáº£: {metadata.get('author', 'N/A')}\n"
                f"Thá»i gian: {metadata.get('time_posted', 'N/A')}\n"
                f"---"
            )
            context_parts.append(context_entry)

        return "\n\n".join(context_parts)

    def create_prompt_template(self, question: str, context: str) -> str:
        """Táº¡o prompt template hiá»‡u quáº£ cho LLM vá»›i cÃ¡c ká»¹ thuáº­t nÃ¢ng cao."""
        if context:
            return f"""Báº¡n lÃ  má»™t chuyÃªn gia phÃ¢n tÃ­ch tin tá»©c AI, cÃ³ nhiá»‡m vá»¥ tá»•ng há»£p thÃ´ng tin má»™t cÃ¡ch khÃ¡ch quan vÃ  chÃ­nh xÃ¡c.
**NGUYÃŠN Táº®C VÃ€NG: CHá»ˆ Ä‘Æ°á»£c phÃ©p sá»­ dá»¥ng thÃ´ng tin tá»« cÃ¡c TÃ€I LIá»†U NGUá»’N Ä‘Æ°á»£c cung cáº¥p dÆ°á»›i Ä‘Ã¢y. NGHIÃŠM Cáº¤M sá»­ dá»¥ng kiáº¿n thá»©c bÃªn ngoÃ i.** Náº¿u thÃ´ng tin khÃ´ng cÃ³ trong tÃ i liá»‡u, hÃ£y tráº£ lá»i rÃµ rÃ ng lÃ : "ThÃ´ng tin nÃ y khÃ´ng cÃ³ trong cÃ¡c tÃ i liá»‡u Ä‘Æ°á»£c cung cáº¥p."

**TÃ€I LIá»†U NGUá»’N:**
---
{context}
---

**CÃ‚U Há»I Cá»¦A NGÆ¯á»œI DÃ™NG:** {question}

**QUY TRÃŒNH SUY LUáº¬N VÃ€ TRáº¢ Lá»œI (HÃ£y tuÃ¢n thá»§ nghiÃªm ngáº·t):**
1.  **PhÃ¢n tÃ­ch cÃ¢u há»i:** Äá»c ká»¹ cÃ¢u há»i Ä‘á»ƒ hiá»ƒu rÃµ yÃªu cáº§u.
2.  **Äá»‘i chiáº¿u vá»›i tÃ i liá»‡u:** Láº§n lÆ°á»£t Ä‘á»c qua tá»«ng [Nguá»“n]. TÃ¬m kiáº¿m cÃ¡c Ä‘oáº¡n vÄƒn báº£n liÃªn quan trá»±c tiáº¿p Ä‘áº¿n cÃ¢u há»i.
3.  **Tá»•ng há»£p vÃ  TrÃ­ch dáº«n:**
    * Tá»•ng há»£p cÃ¡c thÃ´ng tin tÃ¬m Ä‘Æ°á»£c tá»« cÃ¡c nguá»“n khÃ¡c nhau Ä‘á»ƒ táº¡o thÃ nh má»™t cÃ¢u tráº£ lá»i máº¡ch láº¡c, Ä‘áº§y Ä‘á»§.
    * **Báº®T BUá»˜C** pháº£i trÃ­ch dáº«n nguá»“n ngay sau má»—i thÃ´ng tin báº¡n Ä‘Æ°a ra. VÃ­ dá»¥: "Viá»‡t Nam cÃ³ tá»‘c Ä‘á»™ tÄƒng trÆ°á»Ÿng kinh táº¿ áº¥n tÆ°á»£ng trong quÃ½ 1 [Nguá»“n 1], [Nguá»“n 3]."
    * Náº¿u cÃ¡c nguá»“n cÃ³ thÃ´ng tin mÃ¢u thuáº«n, hÃ£y nÃªu rÃµ sá»± mÃ¢u thuáº«n Ä‘Ã³.
4.  **Äá»‹nh dáº¡ng cÃ¢u tráº£ lá»i:**
    * Sá»­ dá»¥ng markdown (gáº¡ch Ä‘áº§u dÃ²ng, in Ä‘áº­m) Ä‘á»ƒ cÃ¢u tráº£ lá»i dá»… Ä‘á»c.
    * Báº¯t Ä‘áº§u báº±ng má»™t cÃ¢u tráº£ lá»i trá»±c tiáº¿p, sau Ä‘Ã³ Ä‘i vÃ o chi tiáº¿t.

**TRáº¢ Lá»œI (theo Ä‘Ãºng quy trÃ¬nh trÃªn):**"""
        else:
            return f"""Báº¡n lÃ  má»™t trá»£ lÃ½ AI thÃ´ng minh vÃ  há»¯u Ã­ch. HÃ£y tráº£ lá»i cÃ¢u há»i sau báº±ng kiáº¿n thá»©c chung cá»§a báº¡n. Náº¿u báº¡n khÃ´ng cháº¯c cháº¯n, hÃ£y nÃ³i ráº±ng báº¡n khÃ´ng biáº¿t.

CÃ‚U Há»I: {question}

TRáº¢ Lá»œI:"""

    # --- THÃŠM HÃ€M Má»šI: ÄÃ¡nh giÃ¡ Ä‘á»™ liÃªn quan cá»§a Context ---
    def _evaluate_context_relevance(self, question: str, context: str) -> bool:
        """
        Sá»­ dá»¥ng LLM Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ xem ngá»¯ cáº£nh Ä‘Æ°á»£c cung cáº¥p cÃ³ Ä‘á»§ thÃ´ng tin
        Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i hay khÃ´ng.
        Tráº£ vá» True náº¿u Ä‘á»§, False náº¿u khÃ´ng.
        """
        if not context:
            return False  # KhÃ´ng cÃ³ context thÃ¬ cháº¯c cháº¯n lÃ  khÃ´ng Ä‘á»§

        eval_prompt = f"""Báº¡n lÃ  má»™t AI Ä‘Ã¡nh giÃ¡. Nhiá»‡m vá»¥ cá»§a báº¡n lÃ  xÃ¡c Ä‘á»‹nh liá»‡u Ä‘oáº¡n TEXT Ä‘Æ°á»£c cung cáº¥p cÃ³ chá»©a Ä‘á»§ thÃ´ng tin Ä‘á»ƒ tráº£ lá»i trá»n váº¹n CÃ‚U Há»I hay khÃ´ng.

**CÃ‚U Há»I:** {question}

**TEXT ÄÆ¯á»¢C CUNG Cáº¤P:**
---
{context}
---

**HÆ¯á»šNG DáºªN:**
-   Náº¿u TEXT chá»©a Ä‘á»§ thÃ´ng tin Ä‘á»ƒ tráº£ lá»i CÃ‚U Há»I, hÃ£y tráº£ lá»i "CÃ“".
-   Náº¿u TEXT KHÃ”NG chá»©a Ä‘á»§ thÃ´ng tin Ä‘á»ƒ tráº£ lá»i CÃ‚U Há»I (vÃ­ dá»¥: thÃ´ng tin khÃ´ng liÃªn quan, quÃ¡ mÆ¡ há»“, thiáº¿u chi tiáº¿t), hÃ£y tráº£ lá»i "KHÃ”NG".
-   KHÃ”NG giáº£i thÃ­ch, KHÃ”NG tráº£ lá»i cÃ¢u há»i, CHá»ˆ tráº£ lá»i "CÃ“" hoáº·c "KHÃ”NG".

**ÄÃP ÃN:**"""

        try:
            response = self.llm_model.invoke(eval_prompt)
            decision = response.content.strip().upper()
            st.info(f"LLM Ä‘Ã¡nh giÃ¡ context: {decision}")  # Debug info
            return decision == "CÃ“"
        except Exception as e:
            st.warning(f"Lá»—i khi LLM Ä‘Ã¡nh giÃ¡ context: {e}. Máº·c Ä‘á»‹nh coi lÃ  khÃ´ng Ä‘á»§.")
            return False  # Náº¿u cÃ³ lá»—i, máº·c Ä‘á»‹nh coi lÃ  context khÃ´ng Ä‘á»§ tá»‘t

    def ask_question(self, question: str) -> Dict[str, Any]:
        """Quy trÃ¬nh hoÃ n chá»‰nh Ä‘á»ƒ há»i vÃ  nháº­n cÃ¢u tráº£ lá»i tá»« chatbot."""
        try:
            query_results = self.get_query_results(question, k=6)

            final_results = []
            context_for_llm = ""
            has_rag_context = False

            if query_results:
                # Giá»›i háº¡n sá»‘ lÆ°á»£ng context Ä‘Æ°a vÃ o LLM Ä‘á»ƒ tá»‘i Æ°u hiá»‡u suáº¥t vÃ  chi phÃ­
                final_results = query_results[:5]
                context_for_llm = self.create_context_from_results(final_results)

                # --- Sá»¬A Äá»”I Táº I ÄÃ‚Y: ThÃªm bÆ°á»›c Ä‘Ã¡nh giÃ¡ ngá»¯ cáº£nh ---
                # Chá»‰ sá»­ dá»¥ng RAG context náº¿u LLM Ä‘Ã¡nh giÃ¡ lÃ  cÃ³ liÃªn quan
                if self._evaluate_context_relevance(question, context_for_llm):
                    prompt = self.create_prompt_template(question, context_for_llm)
                    has_rag_context = True
                    st.info("Sá»­ dá»¥ng RAG context sau khi LLM Ä‘Ã¡nh giÃ¡ Ä‘á»§ cháº¥t lÆ°á»£ng.")
                else:
                    # Náº¿u context RAG khÃ´ng Ä‘á»§ cháº¥t lÆ°á»£ng, bá» qua RAG vÃ  dÃ¹ng kiáº¿n thá»©c chung
                    context_for_llm = ""  # Äáº£m báº£o context rá»—ng Ä‘á»ƒ prompt chá»n nhÃ¡nh else
                    prompt = self.create_prompt_template(question, context_for_llm)
                    st.info("RAG context khÃ´ng Ä‘á»§ cháº¥t lÆ°á»£ng, chuyá»ƒn sang kiáº¿n thá»©c chung.")
            else:
                # Náº¿u khÃ´ng cÃ³ káº¿t quáº£ truy váº¥n nÃ o tá»« Retriever, máº·c Ä‘á»‹nh dÃ¹ng kiáº¿n thá»©c chung
                context_for_llm = ""
                prompt = self.create_prompt_template(question, context_for_llm)
                st.info("KhÃ´ng tÃ¬m tháº¥y RAG context nÃ o tá»« Retriever, sá»­ dá»¥ng kiáº¿n thá»©c chung.")

            response = self.llm_model.invoke(prompt)
            answer = response.content

            # Äáº£m báº£o detailed_sources Ä‘Æ°á»£c táº¡o Ä‘Ãºng cÃ¡ch ngay cáº£ khi khÃ´ng cÃ³ RAG context
            detailed_sources = [{
                "index": i + 1,
                "title": r["metadata"].get("title", "N/A"),
                "url": r["metadata"].get("url", ""),
            } for i, r in enumerate(final_results)] if has_rag_context and final_results else []

            return {
                "answer": answer,
                "detailed_sources": detailed_sources,
                "has_rag_context": has_rag_context,
                "context_used": context_for_llm
            }
        except Exception as e:
            st.error(f"Lá»—i khi xá»­ lÃ½ cÃ¢u há»i: {e}")
            return {
                "answer": "Xin lá»—i, Ä‘Ã£ cÃ³ lá»—i xáº£y ra. Vui lÃ²ng thá»­ láº¡i.",
                "detailed_sources": [],
                "has_rag_context": False,
                "context_used": ""
            }


# --- Cáº¥u hÃ¬nh Streamlit vÃ  Singleton Pattern (Giá»¯ nguyÃªn) ---
@st.cache_resource
def get_chatbot():
    """Táº¡o vÃ  cache singleton chatbot instance."""
    DB_NAME = "vector_db_optimized_v2"
    chatbot = RAGChatbot(db_name=DB_NAME)
    return chatbot


def build_qa_chain():
    """Khá»Ÿi táº¡o há»‡ thá»‘ng RAG vÃ  tráº£ vá» chatbot instance."""
    chatbot = get_chatbot()
    if not chatbot.vectorstore:
        with st.spinner("Äang khá»Ÿi táº¡o há»‡ thá»‘ng RAG... (QuÃ¡ trÃ¬nh nÃ y cÃ³ thá»ƒ máº¥t vÃ i phÃºt náº¿u táº¡o database láº§n Ä‘áº§u)"):
            chatbot.check_and_fix_embedding_dimension()
    return chatbot


# --- CÃ¡c hÃ m tiá»‡n Ã­ch (Giá»¯ nguyÃªn) ---
def ask_chatbot(question: str) -> Dict[str, Any]:
    """HÃ m tiá»‡n Ã­ch Ä‘á»ƒ Ä‘áº·t cÃ¢u há»i cho chatbot."""
    chatbot = get_chatbot()
    return chatbot.ask_question(question)


def get_query_results_debug(question: str, k: int = 5):
    """HÃ m tiá»‡n Ã­ch Ä‘á»ƒ debug, xem trá»±c tiáº¿p káº¿t quáº£ truy váº¥n RAG."""
    chatbot = get_chatbot()
    return chatbot.get_query_results(question, k)