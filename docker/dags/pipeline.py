import sys
import os

# --- Báº¯t Ä‘áº§u pháº§n quáº£n lÃ½ Ä‘Æ°á»ng dáº«n (ÄÃƒ Sá»¬A) ---
# ÄÆ°á»ng dáº«n tá»›i thÆ° má»¥c gá»‘c cá»§a mÃ£ nguá»“n dá»± Ã¡n trong container
# Äáº£m báº£o Ä‘Æ°á»ng dáº«n nÃ y KHá»šP vá»›i vá»‹ trÃ­ báº¡n mount 'sic_project' trong docker-compose.yaml
project_code_root = '/opt/airflow/sic_project'
if project_code_root not in sys.path:
    sys.path.insert(0, project_code_root) # ThÃªm vÃ o Ä‘áº§u sys.path Ä‘á»ƒ Æ°u tiÃªn

# --- Káº¾T THÃšC pháº§n quáº£n lÃ½ Ä‘Æ°á»ng dáº«n ---


# Import cÃ¡c modules cáº§n thiáº¿t tá»« Airflow vÃ  Python chuáº©n
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from airflow.utils.task_group import TaskGroup

# --- Import cÃ¡c modules tá»« dá»± Ã¡n sic_project cá»§a báº¡n (ÄÃƒ Sá»¬A) ---
# VÃ¬ 'project_code_root' (/opt/airflow/sic_project) Ä‘Ã£ Ä‘Æ°á»£c thÃªm vÃ o sys.path,
# báº¡n import trá»±c tiáº¿p tá»« cÃ¡c thÆ° má»¥c con cá»§a nÃ³.
# KHÃ”NG Cáº¦N tiá»n tá»‘ 'sic_project' á»Ÿ Ä‘Ã¢y ná»¯a.
from crawl_data.crawl_data import crawl_all_sites
from process_data.processdt import main as process_main
from process_data.connect_mongo import main as connect_mongo_main # TÃªn file lÃ  connect_mongo.py, hÃ m lÃ  connect_mongo_main

# Äá»‹nh nghÄ©a cÃ¡c arguments máº·c Ä‘á»‹nh
default_args = {
    'owner': 'data-team',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

# Táº¡o DAG
# CHá»ˆ Sá»¬ Dá»¤NG KHá»I `with DAG(...) as dag:` Ä‘á»ƒ Ä‘á»‹nh nghÄ©a Táº¤T Cáº¢ cÃ¡c task vÃ  task group
with DAG(
    dag_id='data_pipeline_dag',
    default_args=default_args,
    description='Pipeline Ä‘á»ƒ crawl data, xá»­ lÃ½ vÃ  lÆ°u vÃ o MongoDB',
    schedule_interval=timedelta(hours=6),  # Cháº¡y má»—i 6 giá»
    catchup=False,
    max_active_runs=1,
    tags=['data', 'pipeline', 'etl']
) as dag: # <-- Báº¯t Ä‘áº§u ngá»¯ cáº£nh cá»§a DAG

    # HÃ m wrapper cho crawl_data (giá»¯ nguyÃªn)
    def run_crawl_data(**context):
        """
        Cháº¡y quÃ¡ trÃ¬nh crawl dá»¯ liá»‡u tá»« táº¥t cáº£ cÃ¡c trang bÃ¡o
        """
        try:
            print("Báº¯t Ä‘áº§u crawl dá»¯ liá»‡u tá»« táº¥t cáº£ cÃ¡c trang bÃ¡o...")
            # Crawl tá»« táº¥t cáº£ cÃ¡c trang vá»›i limit 50 bÃ i/trang
            results = crawl_all_sites(limit=50)

            # Kiá»ƒm tra káº¿t quáº£
            success_count = 0
            total_articles = 0

            for site, result in results.items():
                if result["success"]:
                    success_count += 1
                    total_articles += result["count"]
                    print(f"âœ… {site}: {result['count']} bÃ i bÃ¡o")
                else:
                    print(f"âŒ {site}: {result['error']}")

            print(f"ğŸ“Š Tá»•ng káº¿t: {success_count}/{len(results)} trang thÃ nh cÃ´ng, {total_articles} bÃ i bÃ¡o")
            
            # Tráº£ vá» káº¿t quáº£ cho cÃ¡c task tiáº¿p theo
            context['task_instance'].xcom_push(key='crawl_results', value=results)
            context['task_instance'].xcom_push(key='total_articles', value=total_articles)
            
            return results
            
        except Exception as e:
            print(f"Lá»—i khi crawl data: {str(e)}")
            raise

    # HÃ m wrapper cho process_data (giá»¯ nguyÃªn)
    def run_process_data(**context):
        """
        Cháº¡y quÃ¡ trÃ¬nh xá»­ lÃ½ dá»¯ liá»‡u
        """
        try:
            print("Báº¯t Ä‘áº§u xá»­ lÃ½ dá»¯ liá»‡u...")
            
            # Láº¥y káº¿t quáº£ tá»« task crawl_data (chá»‰ Ä‘á»ƒ in ra log hoáº·c kiá»ƒm tra, khÃ´ng nháº¥t thiáº¿t pháº£i dÃ¹ng náº¿u hÃ m process_main Ä‘á»c tá»« file)
            crawl_results = context['task_instance'].xcom_pull(task_ids='data_processing_group.crawl_data', key='crawl_results')
            total_articles = context['task_instance'].xcom_pull(task_ids='data_processing_group.crawl_data', key='total_articles')
            
            print(f"Nháº­n Ä‘Æ°á»£c {total_articles} bÃ i bÃ¡o tá»« crawl_data (náº¿u cáº§n)")

            result = process_main() # Giáº£ Ä‘á»‹nh process_main Ä‘á»c tá»« file Ä‘Ã£ crawl
            print(f"Xá»­ lÃ½ dá»¯ liá»‡u hoÃ n thÃ nh: {result}")
            
            # Push káº¿t quáº£ cho task tiáº¿p theo
            context['task_instance'].xcom_push(key='process_results', value=result)
            
            return result
        except Exception as e:
            print(f"Lá»—i khi xá»­ lÃ½ dá»¯ liá»‡u: {str(e)}")
            raise

    # HÃ m wrapper cho connect_mongo (giá»¯ nguyÃªn)
    def run_connect_mongo(**context):
        """
        Cháº¡y quÃ¡ trÃ¬nh káº¿t ná»‘i vÃ  lÆ°u dá»¯ liá»‡u vÃ o MongoDB
        """
        try:
            print("Báº¯t Ä‘áº§u káº¿t ná»‘i MongoDB vÃ  lÆ°u dá»¯ liá»‡u...")

            result = connect_mongo_main( # Äáº£m báº£o tÃªn hÃ m Ä‘Ãºng
                input_filename="processed_all_news_combined.json",
                airflow_mongo_conn_id="mongo_atlas_connection",
                database_name="processed_data",
                collection_name="main_data"
            )

            if result:
                print(f"Káº¿t ná»‘i MongoDB hoÃ n thÃ nh vÃ  dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c lÆ°u.")
            else:
                raise Exception("Upload dá»¯ liá»‡u lÃªn MongoDB tháº¥t báº¡i.")

            return result
        except Exception as e:
            print(f"Lá»—i khi káº¿t ná»‘i MongoDB: {str(e)}")
            raise

    # Cáº¥u hÃ¬nh task groups (CHUYá»‚N VÃ€O BÃŠN TRONG KHá»I with DAG)
    with TaskGroup("data_processing_group") as data_group: # Bá» 'dag=dag' á»Ÿ Ä‘Ã¢y, vÃ¬ nÃ³ Ä‘Ã£ náº±m trong ngá»¯ cáº£nh cá»§a DAG
        # Task 1: Crawl Data
        crawl_data_task = PythonOperator(
            task_id='crawl_data',
            python_callable=run_crawl_data,
            doc_md="""
            ### Crawl Data Task
            
            Task nÃ y sáº½:
            - Crawl dá»¯ liá»‡u tá»« cÃ¡c nguá»“n Ä‘Ã£ Ä‘á»‹nh nghÄ©a
            - LÆ°u dá»¯ liá»‡u thÃ´ vÃ o thÆ° má»¥c data
            - Táº¡o log cho quÃ¡ trÃ¬nh crawl
            """,
        )

        # Task 2: Process Data
        process_data_task = PythonOperator(
            task_id='process_data',
            python_callable=run_process_data,
            doc_md="""
            ### Process Data Task
            
            Task nÃ y sáº½:
            - Xá»­ lÃ½ dá»¯ liá»‡u thÃ´ tá»« bÆ°á»›c crawl
            - LÃ m sáº¡ch vÃ  chuáº©n hÃ³a dá»¯ liá»‡u
            - Chuáº©n bá»‹ dá»¯ liá»‡u cho viá»‡c lÆ°u trá»¯
            """,
        )

        # Task 3: Connect MongoDB
        connect_mongo_task = PythonOperator(
            task_id='connect_mongo',
            python_callable=run_connect_mongo,
            doc_md="""
            ### Connect MongoDB Task
            
            Task nÃ y sáº½:
            - Káº¿t ná»‘i tá»›i MongoDB
            - LÆ°u dá»¯ liá»‡u Ä‘Ã£ xá»­ lÃ½ vÃ o database
            - Táº¡o index vÃ  optimize performance
            """,
        )

        # Äá»‹nh nghÄ©a dependencies Ná»˜I Bá»˜ trong nhÃ³m
        crawl_data_task >> process_data_task >> connect_mongo_task

    # Task cleanup (CHUYá»‚N VÃ€O BÃŠN TRONG KHá»I with DAG, bá» dag=dag vÃ¬ nÃ³ tá»± Ä‘á»™ng Ä‘Æ°á»£c gÃ¡n)
    cleanup_task = BashOperator(
        task_id='cleanup_temp_files',
        bash_command="""
        # Dá»n dáº¹p cÃ¡c file táº¡m náº¿u cÃ³
        # ÄÆ°á»ng dáº«n nÃ y cáº§n lÃ  Ä‘Æ°á»ng dáº«n TÆ¯Æ NG Äá»I Tá»ª Gá»C CONTAINER hoáº·c TUYá»†T Äá»I.
        # Náº¿u sic_project Ä‘Æ°á»£c mount vÃ o /opt/airflow/sic_project, thÃ¬ Ä‘Æ°á»ng dáº«n lÃ :
        find /opt/airflow/sic_project/data -name "*.tmp" -delete
        find /opt/airflow/sic_project/logs -name "*.log" -mtime +7 -delete
        """,
    )

    # Task thÃ´ng bÃ¡o hoÃ n thÃ nh (CHUYá»‚N VÃ€O BÃŠN TRONG KHá»I with DAG, bá» dag=dag)
    notification_task = PythonOperator(
        task_id='send_notification',
        python_callable=lambda **context: print("Pipeline hoÃ n thÃ nh thÃ nh cÃ´ng!"),
    )

    # Äá»‹nh nghÄ©a dependencies tá»•ng thá»ƒ cá»§a DAG (giá»¯ nguyÃªn)
    data_group >> cleanup_task >> notification_task

    # ThÃ´ng tin thÃªm cho DAG (giá»¯ nguyÃªn)
    dag.doc_md = """
    # Data Pipeline DAG

    ## MÃ´ táº£
    DAG nÃ y thá»±c hiá»‡n pipeline xá»­ lÃ½ dá»¯ liá»‡u vá»›i cÃ¡c bÆ°á»›c:
    1. **Crawl Data**: Thu tháº­p dá»¯ liá»‡u tá»« cÃ¡c nguá»“n
    2. **Process Data**: Xá»­ lÃ½ vÃ  lÃ m sáº¡ch dá»¯ liá»‡u
    3. **Connect MongoDB**: LÆ°u dá»¯ liá»‡u vÃ o MongoDB

    ## Cáº¥u trÃºc thÆ° má»¥c
    """
# <-- Káº¿t thÃºc ngá»¯ cáº£nh cá»§a DAG